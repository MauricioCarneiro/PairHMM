\documentclass[11pt, oneside]{article}
\usepackage{amsmath}

\title{Enabling high throughput haplotype analysis through hardware acceleration} 
\author{Carneiro MO, Poplin R, Biagioli E, Thibault S, \\  Taylor B, Vacek G, Nehab D, Banks E}

\begin{document} \maketitle
	
	\begin{abstract} 
	
	Identifying insertions and deletions has been the greatest challenge of the 
    new era of high throughput short read DNA sequencing. Although sophisticated
    and accurate statistical methods for indel calling exist, the exhaustive
    computation necessary to correctly evaluate all possible haplotypes against
    the reference hypothesis is a major obstacle to using these algorithms in
    whole genome and large scale complex disease whole exome studies.  Here we
    present two hardware accelerated implementations of the method (as
    implemented in the Genome Analysis Toolkit), that speeds up the parts of the
    algorithm about 300 fold through the use of Graphics Processing Unit (GPU) and
    Floating Poing Gate Array (FPGA) specific code.
    
	
	\end{abstract}

	\section{Introduction} 

    The 1000 Genomes project has set a tremendous stage for the development of
    variant calling methods since it's inception in 2008 [cite original 1kg
    paper]. With the goal of identifying all human variation that is present in
    at least 1\% of the world's population, the project has pushed for more and
    more accurate variant calling programs that were peer reviewed by the
    project and are now available to scientists around the world [cite variant
    callers from 1kg]. After 5 years, the project has achieved over 99\%
    accuracy when calling single nucleotide polymorphisms (SNPs) but still has
    not achieved the same quality in the arguably more difficult insertions and
    deletions (indels) [cite latest 1kg paper].
    
    The Genome Analysis Toolkit (GATK) has recently unveiled a brand new variant
    caller (in the setting of the 1000 Genomes project) that has raised the
    accuracy of indels significantly [cite haplotype caller], but is limited to
    small datasets due to its high computational complexity. The key concept
    behind the GATK's Haplotype Caller is the acknowledgement that short genomic
    rearragements (that will lead to small indels) will be further complicated
    by the complexity of the mapping and alignment of the reads that extend the
    event to the reference genome. From an aligner's perspective, multiple reads
    that span the event are treated as independent observations and may be
    forcefully aligned to the reference genome, creating complex regions that
    are uninteligible to any downstream analyses.

    The Haplotype Caller addresses this inevitable outcome from the alignment
    step by re-assembling these complex regions using denovo assembly algorithms
    that produces a list of candidate haplotypes that would better describe the
    real status of each sample than the reference haplotype. The heart of the
    variant caller lies on assessing, with statistical significance, which one
    of the candidate haplotypes is the most likely to be explained by the read
    data. This natural bayesian view of the data is implemented as a paired
    Hidden Markov Model (pairHMM) that exhaustively calculates the probability
    that each candidate haplotype can be explained by the observed data. The
    most likely haplotypes (depending on ploidy) are then evaluated through an
    exact model to establish the genotype of the sample in ways described
    previously in the GATK's Unified Genotyper [cite GATK] and Samtools [cite
    samtools] variant callers. 

    The Haplotype Caller has brought the indel accuracy from ~60\% to ~97\%
    (show NA12878 vaildation figure), the exhaustive computation nature of its
    pairHMM limits the amount of data that can be processed ($N*M*R*H$ where N
    is the number of reads, M is the number of candidate haplotypes, R is the
    read length and H is candidate haplotype length). However, these
    computations are effectively independent and can be aggressively
    parallelized. Platforms like graphics processing units (GPU) and
    field-programmable gate arrays (FPGA) have the property of allowing high
    parallel computation with minimal overhead enabling us to compute the
    pairHMM not only in parallel, but also parallelizing the computation of
    independent steps of the pairHMM matrix as well.

    Here we show how we can leverage the potential of these hardware accelerated
    platforms to allow us to reduce the runtime of the PairHMM routine inside
    the GATK's Haplotype Caller ~300 fold, and encourage this approach to other
    bioinformatics algorithms by describing our methodology and making the
    source code freely available.

	\section{Methods} 
	
    The pairHMM algorithm takes a list of $N$ reads and a list of $M$ candidate
    haplotypes as input and returns the probability that the data would explain
    each one of the candidate haplotypes. To calculate the probability, the pair
    HMM performs assesses all possible alignments of each haplotype (the
    hypothesis) against all the reads (the data). The HMM has three main states:
    match, insertion and deletion. Here we introduce three concepts related to the
    base quality score ($Q_m$): The insertion gap open penalty (or base
    insertion quality $Q_i$), the deletion gap open penalty (or base deletion
    quality $Q_d$) and the gap continuation penalty ($Q_g$). The first two
    represent the probability that the base precedes an insertion or a deletion
    (respectively) and even when not reported by the instrument can be generated
    by the latest version of the Base Quality Score Recalibration tool
    (BaseRecalibrator) in the GATK [cite GATK]. The third represents the
    probability that an insertion or deletion are extended which in theory could
    be different for insertions and deletions, but in our model we use the same
    flat value $Q10$ for all gap continuation penalties. The pairHMM
    accuracy relies on the accurate estimation of these four quality scores for
    every base. The pairHMM equations are described below:

    \begin{align}
        M_{i,j} &= P_{i,j} (\alpha  M_{i-1,j-1} + \beta  I_{i-1,j-1} + \gamma D_{i-1,j-1} ) \\
        I_{i,j} &= \delta  M_{i-1,j} + \epsilon  I_{i-1,j} \\
        D_{i,j} &= \zeta   M_{i,j-1} + \eta  D_{i,j-1} 
    \end{align}

    Given that $Q_m$ is the base quality score of a base in the read converted
    from phred-scale to probability space using the formula $Q = -log_{10}P$. The
    prior $P_{i,j}$ is $1-Q_m$ of the $i^{th}$ base in the read if it matches
    the $j^{th}$ base in the haplotype, or $Q_m$ if it does not.  The transition
    probabilities are expressed by the roman letters in the equation and are
    described below: 

    \begin{align*}
        \alpha   &= 1 - (Q_i + Q_d) \text{ | match continuation} \\
        \beta    &= 1 - Q_g \text{ | insertion to match} \\
        \gamma   &= 1 - Q_g \text{ | deletion to match} \\
        \delta   &= Q_i \text{ | match to insertion} \\
        \epsilon &= Q_g \text{ | insertion continuation} \\
        \zeta    &= Q_d \text{ | match to deletion} \\
        \eta     &= Q_g \text{ | deletion continuation}
    \end{align*}

    The resulting probability is theoretically the sum of $M_{N,M}$, $I_{N,M}$
    and $D_{N,M}$, however there are a two special conditions in this HMM that
    change:

    \begin{enumerate}
    
        \item To allow the alignment of the haplotype to start anywhere on the read
        without penalty, we need to initialize the entire first row of the
        deletion matrix with the normalized factor
        $\frac{1}{\text{len(}R_i\text{)}}$.

        \item To allow the alignment to end anywhere in the read without penalty, we
        need to ignore the last row of the deletion matrix ($D$) in the final
        calculation of the pairHMM probability.

    \end{enumerate}
	
	\subsection{FPGA Implementation} 
	
	We also investigated optimizing the PairHMM algorithm on Convey Computer’s
	Hybrid-Core (HC) architecture, which previously has proven useful for a
	range of bioinformatics applications including BWA. HC servers combine a
	traditional x86 environment with a high-performance coprocessor based on
	Field Programmable Gate Arrays (FPGAs).  FPGAs are reprogrammable chips that
	can be loaded with instructions for the critical, computationally intensive
	blocks of an algorithm, achieving significant speedups relative to
	general-purpose CPUs. Their ability to be dynamically reprogrammed at run
	time allows them to deliver such optimized performance to many different
	applications.   Coprocessor instruction sets for the HC, or personalities,
	are embedded in applications like the GATK to be automatically used when the
	application runs if a coprocessor is present.  The executable also includes
	the standard x86 instruction set, which is used when running on a
	traditional server where no coprocessor is present.  The paired Hidden
	Markov Model (PairHMM) personality was developed for the HC in Impulse C,
	a personality development productivity tool based on the C language.  The
	PairHMM personality performs comparisons for every base in the read sequence
	in parallel by pipelining the instructions as a systolic array with 162
	elements, which can compare that many bases with a candidate haplotype
	simultaneously.  For the calculation of these direct probabilities, for each
	comparison we calculate the three Viterbi values
	(match, insertion and deletion probabilities) in parallel as well.  The pipeline can be
	envisioned as a wavefront, where the haplotype streams across the sequence
	of read bases to fill out the Viterbi matrix of all comparisons.  The
	processing element emits the full set of Viterbi values for each cell on
	every instruction.  That is, in the first instruction it computes $V_{1,1}$; the
	second computes both $V_{1,2}$ and $V_{2,1}$; the third computes all of $V_{1,3}$, $V_{2,2}$,
	and $V_{3,1}$; etc., with the $j^{th}$ instruction computing $V_{i,j−i+1}$
	for all rows $i$.
	
	For such systolic arrays, top performance is achieved when the pipe remains
	full as many cycles as possible.  In this case, performance is better for
	longer haplotypes, and when reads are as close to 162 bases in length.  The
	read limit is a result of the available resources on the FPGA chips of HC-1
	and HC-2 servers.  HC-2ex servers have a larger coprocessor and would yield
	about a three-fold performance boost over the HC-2.  The additional
	resources could be used for longer read lengths or for more simultaneous
	operations in parallel of the same read lengths with no fundamental change
	in design.  

	
	\subsection{CUDA Implementation} 
	
	Modern GPUs are massively parallel computing devices composed of multiple
	independent multiprocessors, each capable of executing the same operation on
	multiple data elements simultaneously. The architecture is designed for high
	throughput, rather than for low latency.  For best performance, the
	computation must be divided according to this two-level hierarchy: a number
	of independent jobs large enough to keep all multiprocessors busy
	(inter-block parallelism), each job to be executed by collaborating cores
	that perform computations in tandem (intra-block parallelism).

	To this end, we enqueue as many pairs as will fit into the GPU memory before
	launching a persistent kernel.  Each computational block loops obtaining
	pairs from the queue until it is empty. This setup supplies the necessary
	inter-block parallelism.  Due to the restricted amount of shared-memory
	available, and to enable a larger number of resident blocks, we break the
	traversal of matrices~$X$, $Y$ and~$M$ into horizontal bands. Finally, since
	diagonal entries are independent from one another, we evaluate diagonal
	elements in parallel thereby exposing the required intra-block parallelism. 


	\subsection{Experimental data and methods} 
	
    To evaluate the implementations of the pairHMM we made calls on the 1000
    Genomes Project sample NA12878 which is publicly available in the 1000G
    FTP using the Haplotype Caller from GATK version 2.7 and created a test
    dataset to use as input and check the output of the pairHMM routine
    individually (so we could measure the performance improvements directly on
    the pairHMM routine). Our sandboxed implementation of the GATK's PairHMM is
    availabe on github together with the performance evaluation program.
    
    In addition to this test, we also ran the full GATK with the hardware
    acceleration capability to evaluate the impact of the hardware acceleration
    on the Haplotype Caller as a whole (as users will experience). For
    instructions, details and updates on the status of GATK support for these
    and other hardware accelerated platforms, please visit the GATK website on
    http://www.broadinstitute.org/gatk.
	
	\section{Results}


    [ we need a box plot of the performances on the large test for all platforms ]
	
	Performance of the FPGA implementation was measured on an HC-1 with 128GB of
	host memory and 64GB of coprocessor scatter-gather memory with a 2-way
	stripe of 1TB disks.  The host has an L5408 quad core processor running at
	2.13GHz.  The largest data set with 14.1M test cases ran in 770 seconds on
	the HC-1 as compared with about 10800 seconds on a 12-core server with the
	final optimized software (and about 68200 seconds with the original code).
	The PairHMM personality on the HC was implemented with single precision
	instructions.  All results agree with the software to 10**-5 accuracy,
	showing that this is sufficient and double precision instructions are not
	required


	\subsection{Availability of hardware acceleration as a service} 

	Cloud computing offers the ability to flexibly use computing on demand when
	it is needed, and thus to rent it on a per project basis rather than
	purchase servers up front.  It can also provide an opportunity to shift to
	the most appropriate HPC architecture when various applications in a
	workflow would benefit from different resources (coprocessors, high memory,
	etc).  Amazon Web Services (AWS), probably the best known cloud
	service provider, offers both standard x86 servers as well as Cluster GPU
	instances [aws.amazon.com].  Nimbix offers an Accelerated Compute Cloud
	(NACC), with a bioinformatics portal to the widest availability of
	accelerated bioinformatics computing, including Convey HC servers as well as
	standard and GPUs servers [www.nimbix.net].

\end{document}

